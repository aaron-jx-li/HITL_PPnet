{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313c804d-a1d0-4ba1-b2dc-6eca2e5b3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import re\n",
    "from helpers import makedir\n",
    "import model\n",
    "import save\n",
    "from log import create_logger\n",
    "import train_and_test as tnt\n",
    "from helpers import makedir\n",
    "import find_nearest\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from preprocess import mean, std, preprocess_input_function\n",
    "from settings import train_dir, test_dir, train_push_dir\n",
    "from settings import img_size, num_classes, prototype_activation_function, add_on_layers_type\n",
    "\n",
    "from bounding_box_metrics import bounding_box_overlap\n",
    "from find_nearest import find_k_nearest_patches_to_prototypes\n",
    "from settings import coefs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a9163e-50d4-42ff-8bd5-87a96e956910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model initialization for vgg base; do not run this block when creating other models \n",
    "'''\n",
    "\n",
    "base_architecture = 'vgg19'\n",
    "prototype_shape = (2000, 128, 1, 1)\n",
    "\n",
    "ppnet = model.construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "\n",
    "ppnet = ppnet.cuda()\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet).cuda()\n",
    "class_specific = True\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 1e-3,\n",
    "                      'prototype_vectors': 1e-3}\n",
    "warm_optimizer_specs = \\\n",
    "    [{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    "     {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "    ]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 1e-3,\n",
    "                       'prototype_vectors': 1e-3}\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet_multi.module.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet_multi.module.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet_multi.module.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "\n",
    "joint_lr_step_size = 5\n",
    "gamma = 0.5\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.5)\n",
    "\n",
    "last_layer_optimizer_lr = 1e-4\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
    "\n",
    "train_batch_size, test_batch_size, train_push_batch_size = 128, 128, 128\n",
    "push_epochs = [20, 40, 60, 99]\n",
    "push_saved_epochs = [20, 40, 60, 99]\n",
    "\n",
    "model_dir = './ppnet_results/007_vgg19/'\n",
    "makedir(model_dir)\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19960ba1-396d-4a93-8a73-00f3e2a30091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model initialization for resnet base; do not run this block when creating other models \n",
    "'''\n",
    "\n",
    "base_architecture = 'resnet34'\n",
    "prototype_shape = (2000, 256, 1, 1)\n",
    "\n",
    "ppnet = model.construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "\n",
    "ppnet = ppnet.cuda()\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet).cuda()\n",
    "class_specific = True\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 1e-3,\n",
    "                      'prototype_vectors': 1e-3}\n",
    "warm_optimizer_specs = \\\n",
    "    [{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    "     {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "    ]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 1e-3,\n",
    "                       'prototype_vectors': 1e-3}\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet_multi.module.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet_multi.module.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet_multi.module.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "\n",
    "joint_lr_step_size = 5\n",
    "gamma = 0.5\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.5)\n",
    "\n",
    "last_layer_optimizer_lr = 1e-4\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
    "\n",
    "train_batch_size, test_batch_size, train_push_batch_size = 128, 128, 128\n",
    "push_epochs = [20, 40, 60, 99]\n",
    "push_saved_epochs = [20, 40, 60, 99]\n",
    "\n",
    "model_dir = './ppnet_results/005_resnet34/'\n",
    "makedir(model_dir)\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6678d1a2-3ec0-4cca-b8ef-4cb2ddeb4e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model initialization for densenet; do not run this block when creating other models \n",
    "'''\n",
    "\n",
    "base_architecture = 'densenet121'\n",
    "prototype_shape = (2000, 128, 1, 1)\n",
    "\n",
    "ppnet = model.construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "\n",
    "ppnet = ppnet.cuda()\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet).cuda()\n",
    "class_specific = True\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 1e-3,\n",
    "                      'prototype_vectors': 1e-3}\n",
    "warm_optimizer_specs = \\\n",
    "    [{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    "     {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "    ]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 1e-3,\n",
    "                       'prototype_vectors': 1e-3}\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet_multi.module.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet_multi.module.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet_multi.module.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "\n",
    "joint_lr_step_size = 5\n",
    "gamma = 0.5\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.5)\n",
    "\n",
    "last_layer_optimizer_lr = 1e-4\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
    "\n",
    "train_batch_size, test_batch_size, train_push_batch_size = 128, 128, 128\n",
    "push_epochs = [20, 40, 60, 99]\n",
    "push_saved_epochs = [20, 40, 60, 99]\n",
    "\n",
    "model_dir = './ppnet_results/006_densenet121/'\n",
    "makedir(model_dir)\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f361666b-b47f-4876-a34b-0578a40b09f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=mean,\n",
    "                                 std=std)\n",
    "train_dataset = datasets.ImageFolder(\n",
    "        train_dir,\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
    "    num_workers=2, pin_memory=False)\n",
    "\n",
    "# push set\n",
    "train_push_dataset = datasets.ImageFolder(\n",
    "    train_push_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "]))\n",
    "\n",
    "train_push_loader = torch.utils.data.DataLoader(\n",
    "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "\n",
    "# test set\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d04c0e28-7a9c-4a7e-9289-97fb5629a698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: \t0\n",
      "\twarm\n",
      "\ttrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [03:38,  2.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      9\u001b[0m     tnt\u001b[38;5;241m.\u001b[39mwarm_only(model\u001b[38;5;241m=\u001b[39mppnet_multi, log\u001b[38;5;241m=\u001b[39mlog)\n\u001b[0;32m---> 10\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mtnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mppnet_multi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mclass_specific\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     tnt\u001b[38;5;241m.\u001b[39mjoint(model\u001b[38;5;241m=\u001b[39mppnet_multi, log\u001b[38;5;241m=\u001b[39mlog)\n",
      "File \u001b[0;32m~/ProtoPNet/train_and_test.py:133\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, class_specific, coefs, log)\u001b[0m\n\u001b[1;32m    131\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_train_or_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mclass_specific\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ProtoPNet/train_and_test.py:25\u001b[0m, in \u001b[0;36m_train_or_test\u001b[0;34m(model, dataloader, optimizer, class_specific, use_l1_mask, coefs, log)\u001b[0m\n\u001b[1;32m     22\u001b[0m total_separation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m total_avg_separation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (image, label) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#print(i)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     28\u001b[0m     target \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/local/linux/mambaforge-3.11/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 100):\n",
    "    print('epoch: \\t{0}'.format(epoch))\n",
    "\n",
    "    #if epoch in range(0, 10):\n",
    "    #    tnt.joint_warm(model=ppnet_multi, log=log)\n",
    "    #else:\n",
    "    \n",
    "    if epoch in range(0, 5):\n",
    "        tnt.warm_only(model=ppnet_multi, log=log)\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=warm_optimizer,\n",
    "                  class_specific=class_specific, coefs=coefs, log=log)\n",
    "        \n",
    "    else:\n",
    "        tnt.joint(model=ppnet_multi, log=log)\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=joint_optimizer,\n",
    "                  class_specific=class_specific, coefs=coefs, log=log)\n",
    "        joint_lr_scheduler.step()\n",
    "\n",
    "#     accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "#                     class_specific=class_specific, log=log)\n",
    "#     save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n",
    "#                                 target_accu=0.70, log=log)\n",
    "\n",
    "    if epoch >= push_start and epoch in push_epochs:\n",
    "        # Only save the model in in push_saved_epochs\n",
    "        if epoch in push_saved_epochs:\n",
    "            bounding_box_tracker = push.push_prototypes(\n",
    "                train_push_loader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "                prototype_network_parallel=ppnet_multi, # pytorch network with prototype_vectors\n",
    "                class_specific=class_specific,\n",
    "                preprocess_input_function=preprocess_input_function, # normalize if needed\n",
    "                prototype_layer_stride=1,\n",
    "                root_dir_for_saving_prototypes=None, # if not None, prototypes will be saved here\n",
    "                epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n",
    "                prototype_img_filename_prefix=None,\n",
    "                prototype_self_act_filename_prefix=None,\n",
    "                proto_bound_boxes_filename_prefix=None,\n",
    "                save_prototype_class_identity=True,\n",
    "                log=log,\n",
    "                bounding_box_tracker=None)\n",
    "            accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                            class_specific=class_specific, log=log)\n",
    "            save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'push', accu=accu,\n",
    "                                        target_accu=0.72, log=log)\n",
    "\n",
    "           \n",
    "\n",
    "        if prototype_activation_function != 'linear':\n",
    "            tnt.last_only(model=ppnet_multi, log=log)\n",
    "            # Fine tune the last layers\n",
    "            for i in range(8):\n",
    "                log('iteration: \\t{0}'.format(i))\n",
    "                _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n",
    "                              class_specific=class_specific, coefs=coefs, log=log)\n",
    "                accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                    class_specific=class_specific, log=log)\n",
    "                print(\"Test accuracy: \", accu)\n",
    "                save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n",
    "                                target_accu=0.72, log=log)\n",
    "            \n",
    "            save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n",
    "                                target_accu=0.60, log=log)\n",
    "            # Save the last with the final layer fine-tuned\n",
    "        \n",
    "    accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                    class_specific=class_specific, log=log)\n",
    "    print(\"Test accuracy: \", accu)\n",
    "    save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch), accu=accu,\n",
    "                                target_accu=0.72, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfcee4-9274-4efb-8cdf-995a4189de6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
