{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e9f4d784-db44-4a98-b2ce-9bc9dffd01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import numpy as np  \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.distributions.categorical import Categorical\n",
    "import math\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from preprocess import mean, std, preprocess_input_function\n",
    "from settings import train_dir, test_dir, train_push_dir, train_batch_size, test_batch_size, train_push_batch_size\n",
    "from settings import base_architecture, img_size, prototype_shape, num_classes, prototype_activation_function, add_on_layers_type\n",
    "from receptive_field import compute_rf_prototype\n",
    "import cv2\n",
    "from reward_model import construct_PrefNet, paired_cross_entropy_loss, PrefNet\n",
    "from tqdm import tqdm\n",
    "from settings import joint_optimizer_lrs, joint_lr_step_size\n",
    "import skimage as sk\n",
    "import skimage.io as skio\n",
    "import train_and_test as tnt\n",
    "from torch.utils.data import Subset\n",
    "import time\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e66b9c72-2d68-4484-a2a2-e23228001618",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use PPnet's forward pass as the policy network (actor network); what about the network for value function (critic network)?\n",
    "Since there are only determinant actions, this is essentially A2C...\n",
    "'''\n",
    "class A3C_PPnet(nn.Module):\n",
    "    def __init__(self, PPnet, preference_model, k=1, p=5, learning_rate=1e-7, dummy_reward=False, train_batch_size=80):\n",
    "        super(A3C_PPnet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.PPnet = PPnet.cuda()\n",
    "        #for param in self.PPnet.features.parameters():\n",
    "        #    param.requires_grad = True\n",
    "        self.k = k\n",
    "        self.pf_model = preference_model.cuda()\n",
    "        \n",
    "        #self.PPnet_multi = torch.nn.DataParallel(self.PPnet)\n",
    "        self.PPnet_multi = self.PPnet\n",
    "        for p in self.PPnet_multi.module.features.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.PPnet_multi.module.add_on_layers.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.PPnet_multi.module.prototype_vectors.requires_grad = True\n",
    "        for p in self.PPnet_multi.module.last_layer.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        #self.critic_model = self.construct_critic().cuda()\n",
    "        self.p = p\n",
    "        self.critic_model = Critic().cuda()\n",
    "        for p in self.critic_model.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.num_epoch = 0\n",
    "        policy_optimizer_specs = [#{'params': self.PPnet.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, \n",
    "                                  #{'params': self.PPnet.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    "                                  #{'params': self.PPnet.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "                                  #{'params': self.PPnet.add_on_layers.parameters(), 'lr': 1e-6, 'weight_decay': 0},\n",
    "                                  {'params': self.PPnet.module.prototype_vectors, 'lr': 1e-5, 'weight_decay': 1e-3}\n",
    "            \n",
    "                                  ]\n",
    "        self.policy_optimizer = torch.optim.Adam(policy_optimizer_specs)\n",
    "        #self.policy_optimizer = torch.optim.Adam(self.PPnet.features.parameters())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_model.parameters(), lr=1e-4)\n",
    "        self.num_iteration = 0\n",
    "        \n",
    "    def get_heatmaps(self, batch_x, labels, dummy=False, track=False, save_prototypes=[], save_epochs=[]):\n",
    "        self.PPnet_multi.eval()\n",
    "        n_prototypes = self.PPnet_multi.module.num_prototypes\n",
    "        prototype_shape = self.PPnet_multi.module.prototype_shape\n",
    "        max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "        protoL_rf_info = self.PPnet_multi.module.proto_layer_rf_info\n",
    "        \n",
    "        batch_x = batch_x.cuda()\n",
    "        protoL_input_torch, proto_dist_torch = self.PPnet_multi.module.push_forward(batch_x)\n",
    "        \n",
    "        proto_dist_ = proto_dist_torch.view(proto_dist_torch.shape[0], proto_dist_torch.shape[1], -1)\n",
    "        distances = torch.amin(proto_dist_, axis=-1)\n",
    "        #distances = torch.tensor(distances)\n",
    "        #print(\"Distances grad: \", distances.grad)\n",
    "        actions = self.sample_from_distances(distances, labels, track=track)\n",
    "        proto_dist = torch.clone(proto_dist_torch)\n",
    "        # Move to cpu and cast to numpy here\n",
    "        # proto_dist shape: (1000, 80, 7, 7)\n",
    "        proto_dist = torch.transpose(proto_dist, 0, 1)\n",
    "        proto_dist = proto_dist.detach().cpu().numpy()\n",
    "        #heatmaps = []\n",
    "        joint_log_probs = []\n",
    "        original_imgs = []\n",
    "        rescaled_patterns = []\n",
    "        \n",
    "        for action in actions:\n",
    "            img_idx, probs, j, class_identity = action[0], action[1], action[2], action[3]\n",
    "            #heatmaps_j = []\n",
    "            original_imgs_j = []\n",
    "            rescaled_patterns_j = []\n",
    "            #patch_idx_prototype\n",
    "            for i in img_idx:\n",
    "                # patch idx [0-6, 0-6]\n",
    "                closest_patch_indices_in_distance_map_j = list(np.unravel_index(np.argmin(proto_dist[j][i],axis=None), proto_dist[j][i].shape))\n",
    "                closest_patch_indices_in_distance_map_j = [0] + closest_patch_indices_in_distance_map_j\n",
    "                #print(closest_patch_indices_in_distance_map_j)\n",
    "                closest_patch_indices_in_img = compute_rf_prototype(batch_x.size(2), closest_patch_indices_in_distance_map_j, protoL_rf_info)\n",
    "                closest_patch = \\\n",
    "                    batch_x[i, :, closest_patch_indices_in_img[1]:closest_patch_indices_in_img[2], closest_patch_indices_in_img[3]:closest_patch_indices_in_img[4]]\n",
    "                closest_patch = closest_patch.cpu().numpy()\n",
    "                closest_patch = np.transpose(closest_patch, (1, 2, 0))\n",
    "\n",
    "                original_img = batch_x[i].cpu().numpy()\n",
    "                original_img = np.transpose(original_img, (1, 2, 0))\n",
    "                \n",
    "                ## act_pattern is produced by distances of 7x7 patches to the prototype\n",
    "                if self.PPnet_multi.module.prototype_activation_function == 'log':\n",
    "                    act_pattern = np.log((proto_dist[j][i] + 1)/(proto_dist[j][i] + self.PPnet_multi.module.epsilon))\n",
    "                elif self.PPnet_multi.module.prototype_activation_function == 'linear':\n",
    "                    act_pattern = max_dist - proto_dist[j][i]\n",
    "                else:\n",
    "                    act_pattern = prototype_activation_function_in_numpy(proto_dist[j][i])\n",
    "\n",
    "                patch_indices = closest_patch_indices_in_img[1:5]\n",
    "                          \n",
    "                upsampled_act_pattern = cv2.resize(act_pattern, dsize=(img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "                rescaled_act_pattern = upsampled_act_pattern - np.amin(upsampled_act_pattern)\n",
    "                rescaled_act_pattern = rescaled_act_pattern / np.amax(rescaled_act_pattern)\n",
    "                \n",
    "                original_img = original_img - np.amin(original_img)\n",
    "                original_img = original_img / np.amax(original_img)\n",
    "                original_imgs_j.append(original_img)\n",
    "                rescaled_patterns_j.append(rescaled_act_pattern)\n",
    "                #heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_pattern), cv2.COLORMAP_JET)\n",
    "                #heatmap = np.float32(heatmap) / 255\n",
    "                #heatmap = heatmap[..., ::-1]\n",
    "                #overlayed_original_img = 0.5 * original_img + 1.0 * heatmap\n",
    "                #overlayed_original_img = overlayed_original_img - np.amin(overlayed_original_img)\n",
    "                #overlayed_original_img = overlayed_original_img / np.amax(overlayed_original_img)\n",
    "                \n",
    "                \n",
    "            joint_log_prob = torch.prod(probs) * math.factorial(self.k)\n",
    "            original_imgs.append(original_imgs_j)\n",
    "            rescaled_patterns.append(rescaled_patterns_j)\n",
    "            #print(joint_log_prob.grad_fn)\n",
    "            #heatmaps.append(heatmaps_j)\n",
    "            joint_log_probs.append(joint_log_prob)\n",
    "                \n",
    "        # num_prototypes * self.k heatmaps in total\n",
    "        # num_prototypes probs\n",
    "        #for prob in joint_log_probs:\n",
    "        #    print(prob.grad_fn)\n",
    "    \n",
    "        return original_imgs, rescaled_patterns, joint_log_probs, distances\n",
    "    \n",
    "    def sample_from_distances(self, distances, labels, track_iters=[], track=False):\n",
    "        '''\n",
    "        Takes in distances of shape (80, 1000)\n",
    "        returns actions of shape (1000, ), one for each prototype\n",
    "        '''\n",
    "        \n",
    "        distances = torch.clip(distances, min=1e-7, max=None)\n",
    "        similarities = 1 / distances\n",
    "        #print(similarities)\n",
    "        softmax_dist = F.log_softmax(similarities, dim=0)\n",
    "        softmax_dist = torch.transpose(softmax_dist, 0, 1)\n",
    "        # Maybe using combinatorics?\n",
    "        actions = []\n",
    "        # For each of the 1000 prototypes...\n",
    "        for i in range(softmax_dist.shape[0]):\n",
    "            class_identity = torch.argmax(self.PPnet_multi.module.prototype_class_identity[i])\n",
    "            #print(class_identity, class_identity.shape)\n",
    "            class_dist = softmax_dist[i][labels==class_identity]\n",
    "            \n",
    "            #print(class_dist, class_dist.shape)\n",
    "            if len(class_dist) > self.k:\n",
    "                #print(class_dist)\n",
    "                dist = Categorical(class_dist)\n",
    "                img_idx = dist.sample(sample_shape=torch.tensor([self.k]))\n",
    "                probs = dist.log_prob(img_idx)\n",
    "                probs = torch.exp(probs)\n",
    "                actions.append([img_idx, probs, i, class_identity])\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "    # Currently just the same architecture as the pref_net\n",
    "    '''\n",
    "    def critic(self, heatmaps):\n",
    "        values = torch.empty(len(heatmaps))\n",
    "        for i in tqdm(range(len(heatmaps))):\n",
    "            #x = torch.tensor(heatmaps[i])\n",
    "            x = np.concatenate(heatmaps[i], axis=1)\n",
    "            x = torch.tensor(x).cuda()\n",
    "            x = torch.unsqueeze(x, axis=0)\n",
    "            x = torch.transpose(x, 1, 3)\n",
    "            with torch.no_grad():\n",
    "                x = self.pf_model.conv_features(x)\n",
    "                x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = self.critic_model(x)\n",
    "            values[i] = x\n",
    "            #print(i)\n",
    "        return values\n",
    "    '''\n",
    "    \n",
    "    # Need to vectorize\n",
    "    def get_critic_inputs(self, imgs, patterns, dummy=False):\n",
    "        critic_inputs = []\n",
    "        for i in range(len(imgs)):\n",
    "            #x = np.concatenate(heatmaps[i], axis=1)\n",
    "            #print(x.shape)\n",
    "            x = imgs[i]\n",
    "            \n",
    "            x = np.transpose(x, (0, 3, 1, 2))\n",
    "            x = torch.tensor(x).cuda()\n",
    "            # x shape: (1, 3, 224, 224)\n",
    "            #x = torch.unsqueeze(x, axis=0)\n",
    "            \n",
    "            p = patterns[i][0]\n",
    "            p = np.array([p, p, p])\n",
    "            p = torch.from_numpy(np.array([p])).cuda()\n",
    "            # p shape: (1, 3, 224, 224)\n",
    "            with torch.no_grad():\n",
    "                x = self.pf_model.conv_features(x)\n",
    "                p = self.pf_model.conv_features(p)\n",
    "            \n",
    "            critic_inputs.append((x, p))\n",
    "       \n",
    "        return critic_inputs\n",
    "        \n",
    "    def critic(self, critic_inputs):\n",
    "        values = torch.zeros(len(critic_inputs))\n",
    "        for i in range(len(critic_inputs)):\n",
    "            value = self.critic_model(critic_inputs[i][0], critic_inputs[i][1])\n",
    "            values[i] = value\n",
    "        return values\n",
    "    \n",
    "    '''\n",
    "    imgs: (1000, 1, 224, 224, 3), patterns: (1000, 1, 224, 224)\n",
    "    '''\n",
    "    def get_rewards(self, imgs, patterns, dummy=False):\n",
    "        with torch.no_grad():\n",
    "            rewards = torch.empty(len(imgs))\n",
    "            for i in range(len(imgs)):\n",
    "                '''\n",
    "                pf_input = torch.tensor(np.array(heatmaps[i])).cuda()\n",
    "                pf_input = pf_input.view(pf_input.shape[0]*pf_input.shape[1], pf_input.shape[2], pf_input.shape[3])\n",
    "                pf_input = torch.transpose(pf_input, 0, 2)\n",
    "                pf_input = torch.transpose(pf_input, 1, 2)\n",
    "                pf_input = torch.unsqueeze(pf_input, axis=0)\n",
    "                '''\n",
    "                \n",
    "                img = np.array(imgs[i])\n",
    "                img = np.transpose(img, (0, 3, 1, 2))\n",
    "                img = torch.from_numpy(img).cuda()\n",
    "                \n",
    "                pattern = patterns[i][0]\n",
    "                pattern = np.array([pattern, pattern, pattern])\n",
    "                pattern = torch.from_numpy(np.array([pattern])).cuda()\n",
    "                reward = self.pf_model(img, pattern) + 15\n",
    "                rewards[i] = reward\n",
    "                \n",
    "        return rewards\n",
    "        \n",
    "    def update_v1(self, rewards, values, probs):\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss = 0\n",
    "        '''\n",
    "        Customized A2C\n",
    "        '''\n",
    "        for i in range(len(rewards)):\n",
    "            policy_loss -= probs[i] * (rewards[i] - values[i])    \n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = 0\n",
    "        for i in range(len(rewards)):\n",
    "            critic_loss += (rewards[i] - values[i]) ** 2\n",
    "        \n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return \n",
    "    \n",
    "    \n",
    "    def run(self, batch_x, labels, save_prototypes=[], save_epochs=[], track=False):\n",
    "\n",
    "        # action is n_prototypes * k heatmaps\n",
    "        original_imgs, rescaled_patterns, probs, img_distances = self.get_heatmaps(batch_x, labels, track=track)\n",
    "        \n",
    "        critic_inputs = self.get_critic_inputs(original_imgs, rescaled_patterns)\n",
    "        values = self.critic(critic_inputs)\n",
    "        \n",
    "        rewards = self.get_rewards(original_imgs, rescaled_patterns)\n",
    "        \n",
    "        self.update_v1(rewards, values, probs)\n",
    "        \n",
    "        self.num_iteration += 1\n",
    "        if self.num_iteration == 75:\n",
    "            self.num_iteration = 0\n",
    "            self.num_epoch += 1\n",
    "        \n",
    "        return rewards, values, probs\n",
    "    '''\n",
    "    \n",
    "    def run_dummy(self, batch_x, labels, save_prototypes=[], save_epochs=[], track=False):\n",
    "        heatmaps, probs, img_distances, rewards  = self.get_heatmaps(batch_x, labels, dummy=True, track=track, save_prototypes=save_prototypes, save_epochs=save_epochs)\n",
    "        \n",
    "        critic_inputs = self.get_critic_inputs(heatmaps)\n",
    "        values = self.critic_model(critic_inputs)\n",
    "        #values = []\n",
    "        self.update_v1(rewards, values, probs)\n",
    "        #if len(save_prototypes) > 0 and self.num_iteration in save_iters:\n",
    "        #    for p in save_prototypes:\n",
    "        #        \n",
    "        #        for k in range(len(heatmaps[p])):\n",
    "        #            plt.imsave(r'./A3C_results/epoch_'+str(self.num_epoch)+'_prototype_'+str(p)+'_best_'+str(k+1)+'_dummy.jpg', heatmaps[p][k])\n",
    "            \n",
    "        self.num_iteration += 1\n",
    "        if self.num_iteration == 75:\n",
    "            self.num_iteration = 0\n",
    "            self.num_epoch += 1\n",
    "        \n",
    "        return rewards, values, probs, heatmaps\n",
    "    '''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5c9b1871-610e-4893-b400-bf453e392f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, k=3, learning_rate=3e-4):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        '''\n",
    "        \n",
    "        self.img_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.pattern_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.fc1 = nn.Linear(6400, 512)\n",
    "        self.fc2 = nn.Linear(512, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x ,p):\n",
    "\n",
    "        x = self.img_conv(x)\n",
    "        p = self.pattern_conv(p)\n",
    "    \n",
    "        out = torch.cat((x, p), dim=1)\n",
    "        out = torch.flatten(out, 1) # flatten all dimensions except batch\n",
    "\n",
    "        out = torch.sigmoid(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1638943a-ba3e-453b-a34b-7d050a4142c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reselect_prototypes(a3c, reward_threshold, data_loader, original_imgs, patterns):\n",
    "    # get the heatmaps by searching for the closest images in the entire dataset\n",
    "    # can use different k values\n",
    "    # heatmaps: (1000, 1, 224, 224, 3)\n",
    "    # rewards: (1000,)\n",
    "    prototype_shape = a3c.PPnet_multi.module.prototype_shape\n",
    "    bad_prototype_idx = []\n",
    "    rewards = a3c.get_rewards(original_imgs, patterns)\n",
    "    for i in range(len(rewards)):\n",
    "        if rewards[i] < reward_threshold:\n",
    "            bad_prototype_idx.append(i)\n",
    "    \n",
    "    global_max_rewards = np.zeros((200, 5))\n",
    "    global_best_patches = torch.zeros((200, 5, 128, 1, 1)).cuda()\n",
    "    \n",
    "    class_comps = np.zeros((200, 5))\n",
    "    \n",
    "    for idx, (batch_x, labels) in tqdm(enumerate(data_loader)):\n",
    "        a3c.PPnet_multi.eval()\n",
    "        n_prototypes = a3c.PPnet_multi.module.num_prototypes\n",
    "        prototype_shape = a3c.PPnet_multi.module.prototype_shape\n",
    "        max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "        protoL_rf_info = a3c.PPnet_multi.module.proto_layer_rf_info\n",
    "        \n",
    "        batch_x = batch_x.cuda()\n",
    "        # conv_outs: (80, 128, 7, 7)\n",
    "        conv_outs = a3c.PPnet_multi.module.conv_features(batch_x)\n",
    "\n",
    "        # loop over each prototype\n",
    "        for j in bad_prototype_idx:\n",
    "            class_identity = torch.argmax(a3c.PPnet_multi.module.prototype_class_identity[j])\n",
    "            batch_idx = np.arange(batch_x.shape[0])\n",
    "            class_idx_batch = batch_idx[labels == class_identity]\n",
    "            #print(class_idx_batch)\n",
    "            class_outs = conv_outs[labels == class_identity]\n",
    "            if class_outs.shape[0] == 0:\n",
    "                continue\n",
    "            height = class_outs.shape[2]\n",
    "            width = class_outs.shape[3]\n",
    "            \n",
    "            for img_idx in range(class_outs.shape[0]):\n",
    "                img_max_reward = 0\n",
    "                img_best_patch = a3c.PPnet_multi.module.prototype_vectors.data[j]\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        # actually old_vec might not be necessary\n",
    "                        # old_vec = np.copy(a3c.PPnet_multi.module.prototype_vectors.data[j])\n",
    "                        patch_candidate = class_outs[img_idx, :, h:h+1, w:w+1]\n",
    "                        a3c.PPnet_multi.module.prototype_vectors.data[j] = patch_candidate\n",
    "                        #a3c.PPnet_multi.module.prototype_vectors.data.copy_(vec)\n",
    "                        # distances: (7, 7)\n",
    "                        distances = a3c.PPnet_multi.module._l2_convolution(class_outs[img_idx])[j].detach().cpu().numpy()\n",
    "                        \n",
    "                        if a3c.PPnet_multi.module.prototype_activation_function == 'log':\n",
    "                            act_pattern = np.log((distances + 1)/(distances + a3c.PPnet_multi.module.epsilon))\n",
    "                        elif a3c.PPnet_multi.module.prototype_activation_function == 'linear':\n",
    "                            act_pattern = max_dist - distances\n",
    "                        else:\n",
    "                            act_pattern = prototype_activation_function_in_numpy(distances)\n",
    "                            \n",
    "                        upsampled_act_pattern = cv2.resize(act_pattern, dsize=(img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
    "                        rescaled_act_pattern = upsampled_act_pattern - np.amin(upsampled_act_pattern)\n",
    "                        rescaled_act_pattern = rescaled_act_pattern / np.amax(rescaled_act_pattern)\n",
    "                        \n",
    "                        img = torch.unsqueeze(batch_x[class_idx_batch[img_idx]], 0)\n",
    "                        pattern = torch.tensor([rescaled_act_pattern, rescaled_act_pattern, rescaled_act_pattern])\n",
    "                        pattern = torch.unsqueeze(pattern, 0)\n",
    "                        \n",
    "                        \n",
    "                        patch_reward = a3c.pf_model(img, pattern.cuda()) + 15\n",
    "                        \n",
    "                        if patch_reward > img_max_reward:\n",
    "                            img_max_reward = patch_reward\n",
    "                            img_best_patch = patch_candidate\n",
    "                        #else:\n",
    "                        #    a3c.PPnet_multi.module.prototype_vectors.data[j] = old_vec\n",
    "                            \n",
    "                \n",
    "                min_index = int(class_comps[class_identity][-1])\n",
    "                if img_max_reward > global_max_rewards[class_identity][min_index]:\n",
    "                    global_max_rewards[class_identity][min_index] = img_max_reward\n",
    "                    global_best_patches[class_identity][min_index] = img_best_patch\n",
    "                    class_comps[class_identity] = np.flip(np.argsort(global_max_rewards[class_identity]))\n",
    "                   \n",
    "    #print(class_comps)\n",
    "    #print(global_max_rewards)\n",
    "    for i in bad_prototype_idx:\n",
    "        class_num = int(i // 5)\n",
    "        p_num = int(class_comps[class_num][0])\n",
    "        class_comps[class_num] = np.roll(class_comps[class_num], -1)\n",
    "        a3c.PPnet_multi.module.prototype_vectors.data[i] = global_best_patches[class_num][p_num]\n",
    "        \n",
    "    return global_max_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c9b4f3e6-660e-43d3-8ff2-5a5499df3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatch:\n",
    "\n",
    "    def __init__(self, patch, label, distance,\n",
    "                 original_img=None, act_pattern=None, patch_indices=None):\n",
    "        self.patch = patch\n",
    "        self.label = label\n",
    "        self.negative_distance = -distance\n",
    "\n",
    "        self.original_img = original_img\n",
    "        self.act_pattern = act_pattern\n",
    "        self.patch_indices = patch_indices\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.negative_distance < other.negative_distance\n",
    "\n",
    "\n",
    "class ImagePatchInfo:\n",
    "\n",
    "    def __init__(self, label, distance):\n",
    "        self.label = label\n",
    "        self.negative_distance = -distance\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.negative_distance < other.negative_distance\n",
    "\n",
    "def find_k_nearest_patches_to_prototypes(dataloader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "                                         prototype_network_parallel, # pytorch network with prototype_vectors\n",
    "                                         k=3,\n",
    "                                         preprocess_input_function=None, # normalize if needed\n",
    "                                         full_save=False, # save all the images\n",
    "                                         root_dir_for_saving_images='./nearest',\n",
    "                                         log=print,\n",
    "                                         prototype_activation_function_in_numpy=None, heatmap_ratio = 1.0):\n",
    "    prototype_network_parallel.eval()\n",
    "    '''\n",
    "    full_save=False will only return the class identity of the closest\n",
    "    patches, but it will not save anything.\n",
    "    '''\n",
    "    log('find nearest patches')\n",
    "    start = time.time()\n",
    "    n_prototypes = prototype_network_parallel.module.num_prototypes\n",
    "    \n",
    "    prototype_shape = prototype_network_parallel.module.prototype_shape\n",
    "    max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "\n",
    "    protoL_rf_info = prototype_network_parallel.module.proto_layer_rf_info\n",
    "\n",
    "    heaps = []\n",
    "    # allocate an array of n_prototypes number of heaps\n",
    "    for _ in range(n_prototypes):\n",
    "        # a heap in python is just a maintained list\n",
    "        heaps.append([])\n",
    "\n",
    "    for idx, (search_batch_input, search_y) in tqdm(enumerate(dataloader)):\n",
    "        #print('batch {}'.format(idx))\n",
    "        if preprocess_input_function is not None:\n",
    "            # print('preprocessing input for pushing ...')\n",
    "            # search_batch = copy.deepcopy(search_batch_input)\n",
    "            search_batch = preprocess_input_function(search_batch_input)\n",
    "\n",
    "        else:\n",
    "            search_batch = search_batch_input\n",
    "\n",
    "        with torch.no_grad():\n",
    "            search_batch = search_batch.cuda()\n",
    "            protoL_input_torch, proto_dist_torch = \\\n",
    "                prototype_network_parallel.module.push_forward(search_batch)\n",
    "\n",
    "        #protoL_input_ = np.copy(protoL_input_torch.detach().cpu().numpy())\n",
    "        proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
    "        \n",
    "        # proto_dist_: (80, 1000, 7, 7)\n",
    "\n",
    "        for img_idx, distance_map in enumerate(proto_dist_):\n",
    "            for j in range(n_prototypes):\n",
    "                # find the closest patches in this batch to prototype j\n",
    "\n",
    "                closest_patch_distance_to_prototype_j = np.amin(distance_map[j])\n",
    "\n",
    "\n",
    "\n",
    "                closest_patch_indices_in_distance_map_j = \\\n",
    "                    list(np.unravel_index(np.argmin(distance_map[j],axis=None),\n",
    "                                          distance_map[j].shape))\n",
    "                closest_patch_indices_in_distance_map_j = [0] + closest_patch_indices_in_distance_map_j\n",
    "                closest_patch_indices_in_img = \\\n",
    "                    compute_rf_prototype(search_batch.size(2),\n",
    "                                         closest_patch_indices_in_distance_map_j,\n",
    "                                         protoL_rf_info)\n",
    "                closest_patch = \\\n",
    "                    search_batch_input[img_idx, :,\n",
    "                                       closest_patch_indices_in_img[1]:closest_patch_indices_in_img[2],\n",
    "                                       closest_patch_indices_in_img[3]:closest_patch_indices_in_img[4]]\n",
    "                closest_patch = closest_patch.numpy()\n",
    "                closest_patch = np.transpose(closest_patch, (1, 2, 0))\n",
    "\n",
    "                original_img = search_batch_input[img_idx].numpy()\n",
    "                original_img = np.transpose(original_img, (1, 2, 0))\n",
    "\n",
    "                if prototype_network_parallel.module.prototype_activation_function == 'log':\n",
    "                    act_pattern = np.log((distance_map[j] + 1)/(distance_map[j] + prototype_network_parallel.module.epsilon))\n",
    "                elif prototype_network_parallel.module.prototype_activation_function == 'linear':\n",
    "                    act_pattern = max_dist - distance_map[j]\n",
    "                else:\n",
    "                    act_pattern = prototype_activation_function_in_numpy(distance_map[j])\n",
    "\n",
    "                # 4 numbers: height_start, height_end, width_start, width_end\n",
    "                patch_indices = closest_patch_indices_in_img[1:5]\n",
    "\n",
    "                # construct the closest patch object\n",
    "                closest_patch = ImagePatch(patch=closest_patch,\n",
    "                                           label=search_y[img_idx],\n",
    "                                           distance=closest_patch_distance_to_prototype_j,\n",
    "                                           original_img=original_img,\n",
    "                                           act_pattern=act_pattern,\n",
    "                                           patch_indices=patch_indices)\n",
    "                '''\n",
    "                else:\n",
    "                \n",
    "                closest_patch = ImagePatchInfo(label=search_y[img_idx],\n",
    "                                                   distance=closest_patch_distance_to_prototype_j)\n",
    "                '''\n",
    "\n",
    "                # add to the j-th heap \n",
    "                if len(heaps[j]) < k:\n",
    "                    heapq.heappush(heaps[j], closest_patch)\n",
    "                else:\n",
    "                    # heappushpop runs more efficiently than heappush\n",
    "                    # followed by heappop\n",
    "                    heapq.heappushpop(heaps[j], closest_patch)\n",
    "                    \n",
    "\n",
    "    # after looping through the dataset every heap will\n",
    "    # have the k closest prototypes\n",
    "    original_imgs = []\n",
    "    patterns = []\n",
    "    for j in range(n_prototypes):\n",
    "        # finally sort the heap; the heap only contains the k closest\n",
    "        # but they are not ranked yet\n",
    "        heaps[j].sort()\n",
    "        heaps[j] = heaps[j][::-1]\n",
    "\n",
    "        original_imgs_j = []\n",
    "        patterns_j = []\n",
    "        for i, patch in enumerate(heaps[j]):\n",
    "            \n",
    "            img_size = patch.original_img.shape[0]\n",
    "            upsampled_act_pattern = cv2.resize(patch.act_pattern,\n",
    "                                               dsize=(img_size, img_size),\n",
    "                                               interpolation=cv2.INTER_CUBIC)\n",
    "            rescaled_act_pattern = upsampled_act_pattern - np.amin(upsampled_act_pattern)\n",
    "            rescaled_act_pattern = rescaled_act_pattern / np.amax(rescaled_act_pattern)\n",
    "            \n",
    "            # No need for these if using dummy reward model\n",
    "            '''\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_pattern), cv2.COLORMAP_JET)\n",
    "            heatmap = np.float32(heatmap) / 255\n",
    "            heatmap = heatmap[...,::-1]\n",
    "\n",
    "            overlayed_original_img = 0.5 * patch.original_img + heatmap_ratio * heatmap\n",
    "            overlayed_original_img = overlayed_original_img - np.amin(overlayed_original_img)\n",
    "            overlayed_original_img = overlayed_original_img / np.amax(overlayed_original_img)\n",
    "            '''\n",
    "            original_imgs_j.append(patch.original_img)\n",
    "            patterns_j.append(rescaled_act_pattern)\n",
    "            \n",
    "        original_imgs.append(original_imgs_j)\n",
    "        patterns.append(patterns_j)\n",
    "    end = time.time()\n",
    "    log('\\tfind nearest patches time: \\t{0}'.format(end - start))\n",
    "\n",
    "    return original_imgs, patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4577affc-d419-47a7-a23d-cbb8f65f4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dummy(a3c, ac_loader, reselection_loader):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8a3f6116-dbd6-480b-aa3a-703d8e9d66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnet = torch.load(r'../saved_models/vgg19/004/100_7push0.7344.pth')\n",
    "ppnet = torch.nn.DataParallel(ppnet)\n",
    "#pf_model = construct_PrefNet(\"resnet18\")\n",
    "#pf_model.load_state_dict(torch.load(\"./human_comparisons/pref_model_009_65+35_ep50_adam_0.0001\"))\n",
    "#pf_model = torch.load(r'./human_comparisons/pref_model_009_65+35_ep50_adam_0.0001_1')\n",
    "pf_model = torch.load('./human_comparisons/pref_model_500rating_split0.7_acc0.82.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cac37359-f88a-41bc-9c0e-403d44aadc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc = tnt.test(model=torch.nn.DataParallel(ppnet), dataloader=test_loader, class_specific=True, log=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7d15a4c1-4e13-49c0-a9c4-27632b8a0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "        train_push_dir,\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=80, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "        test_dir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "#shuffled_dataset = Subset(train_dataset, shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "92711ab6-79f9-43f0-8130-204c0bacae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "\n",
    "'''\n",
    "Each batch of size 80 consists of 16 shuffled blocks\n",
    "'''\n",
    "for i in range(200):\n",
    "    class_i = [ind for ind, ele in enumerate(train_dataset.targets) if ele == i]\n",
    "    indices.append(class_i[:5])\n",
    "    indices.append(class_i[5:10])\n",
    "    indices.append(class_i[10:15])\n",
    "    indices.append(class_i[15:20])\n",
    "    indices.append(class_i[20:25])\n",
    "    indices.append(class_i[25:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "45306fbb-504b-429d-9e67-ff775cd66067",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3c = A3C_PPnet(ppnet, pf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7d7ee7d1-7921-4477-aede-bd84dcf7943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward_006 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b410c43-9ec5-4b5b-9aa9-60a8724fe573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [01:56,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 74 average reward:  901.5517248205236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [02:02,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 74 average reward:  899.5642378523543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [02:04,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 74 average reward:  880.5211598164326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:58,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 74 average reward:  880.8933179700697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:59,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 74 average reward:  927.5155738624367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:57,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 74 average reward:  880.9620650007918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:57,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 74 average reward:  911.5655047442463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:59,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 74 average reward:  918.3127086742504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [01:59,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 74 average reward:  940.5995425662479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "42it [01:06,  1.53s/it]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Human feedback training with reselection\n",
    "'''\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    if epoch in [20, 50]:\n",
    "        original_imgs, patterns = find_k_nearest_patches_to_prototypes(train_loader, a3c.PPnet_multi, k=1)\n",
    "        global_max_rewards = reselect_prototypes(a3c, 0, train_loader, original_imgs, patterns)\n",
    "    order = np.random.permutation(1200)\n",
    "    shuffled_idx = []\n",
    "    for idx in order:\n",
    "        shuffled_idx += indices[idx]\n",
    "        \n",
    "    shuffled_dataset = Subset(train_dataset, shuffled_idx)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "    shuffled_dataset, batch_size=80, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "    \n",
    "    epoch_reward = 0\n",
    "    \n",
    "    for i, (batch, labels) in tqdm(enumerate(dataloader)):\n",
    "        rewards, values, probs = a3c.run(batch, labels, save_prototypes=[], save_epochs=[])\n",
    "        \n",
    "        total_reward = 0\n",
    "        mse_loss = 0\n",
    "        for j in range(len(probs)):\n",
    "            \n",
    "            probs[j].detach().cpu().numpy()\n",
    "            rewards[j].detach().cpu().numpy()\n",
    "            #total_reward += probs[j] * rewards[j]\n",
    "            #mse_loss += (rewards[j] - values[j]) ** 2\n",
    "            total_reward += rewards[j]\n",
    "            \n",
    "            #probs[j].detach().cpu().numpy()\n",
    "            #rewards[j].detach().cpu().numpy()\n",
    "        epoch_reward += total_reward.item()\n",
    "        #print(epoch_reward)\n",
    "        #print(\"Epoch \"+str(epoch)+\" Iteration \"+str(i)+\" total expected reward: \", total_reward)\n",
    "        #print(\"Epoch \"+str(epoch)+\" Iteration \"+str(i)+\" total mse loss: \", mse_loss)\n",
    "    \n",
    "    print(\"Epoch \"+str(a3c.num_epoch)+\" \"+str(i)+\" average reward: \", epoch_reward/i)\n",
    "    \n",
    "    avg_reward_006.append(epoch_reward/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6aa79a44-195f-4e1d-bc41-2078090f4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a3c.PPnet_multi, r'./A3C_results/005_ppnet.pth')\n",
    "torch.save(a3c.critic_model, r'./A3C_results/005_critic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29f51ef8-b63f-47ff-9b8a-aa0a3ced0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./A3C_results/003_dummy_epoch_rewards_003.npy', avg_reward_dummy500_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8cb701b-49e9-44d4-9409-28cc3e6356c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4a9837bdf0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(515), avg_reward_dummy500_003)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
